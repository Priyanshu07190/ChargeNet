import { useState, useEffect, useRef, useCallback } from 'react';
import { Mic, X, Sparkles, Volume2, Settings, Send } from 'lucide-react';
import { useAuth } from '../contexts/AuthContext';
import { useNavigate } from 'react-router-dom';
import { chatWithGemini, extractAction, resetConversation } from '../lib/geminiService';
import { executeAction, VoiceActionContext } from '../lib/voiceActionEngine';
import { smartSpeak } from '../lib/elevenlabsService';
import { initVAD, stopVAD } from '../lib/voiceActivityDetection';
import {
  hasTrainedModel,
  loadAndListen,
  pauseListening,
  resumeListening,
  cleanup as cleanupWakeWord,
  resetWakeWord,
} from '../lib/wakeWordEngine';
import WakeWordSetup from './WakeWordSetup';

interface Message {
  id: string;
  text: string;
  sender: 'user' | 'gennie';
  timestamp: Date;
}

/**
 * Professional Voice Assistant
 * Uses Voice Activity Detection (VAD) like real AI assistants
 */
export default function GenniePro() {
  const [isOpen, setIsOpen] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isUserSpeaking, setIsUserSpeaking] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const [messages, setMessages] = useState<Message[]>([]);
  const [isProcessing, setIsProcessing] = useState(false);
  const [currentTranscript, setCurrentTranscript] = useState('');
  const [wakeWordReady, setWakeWordReady] = useState(false);
  const [showWakeWordSetup, setShowWakeWordSetup] = useState(false);
  const [textInput, setTextInput] = useState('');
  const [micSupported, setMicSupported] = useState(true);
  const [vadSupported, setVadSupported] = useState(true);
  
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const recognitionRef = useRef<any>(null);
  const isRecognitionActive = useRef(false);
  const pendingTranscript = useRef('');
  const isOpenRef = useRef(false); // tracks isOpen for callbacks
  const { user } = useAuth();
  const navigate = useNavigate();

  // Keep ref in sync with state
  useEffect(() => {
    isOpenRef.current = isOpen;
  }, [isOpen]);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  const addUserMessage = (text: string) => {
    const message: Message = {
      id: Date.now().toString(),
      text,
      sender: 'user',
      timestamp: new Date()
    };
    setMessages(prev => [...prev, message]);
  };

  const speak = useCallback(async (text: string) => {
    try {
      setIsSpeaking(true);
      console.log('üîä Gennie speaking:', text);
      await smartSpeak(text);
      setIsSpeaking(false);
      console.log('‚úÖ Finished speaking');
    } catch (error) {
      console.error('Speech error:', error);
      setIsSpeaking(false);
    }
  }, []);

  const addGenieMessage = useCallback((text: string) => {
    const message: Message = {
      id: Date.now().toString(),
      text,
      sender: 'gennie',
      timestamp: new Date()
    };
    setMessages(prev => [...prev, message]);
    speak(text);
  }, [speak]);

  const processCommand = useCallback(async (userInput: string) => {
    console.log('ü§ñ Processing:', userInput);

    try {
      const aiResponse = await chatWithGemini(userInput, user?.user_type as 'driver' | 'host');
      const actionData = extractAction(aiResponse);
      // Strip ACTION:... from display text (handles values with any characters)
      const cleanResponse = aiResponse.replace(/\s*ACTION:[A-Z_]+(?::[^\s]*)?/g, '').trim();
      
      if (actionData) {
        console.log('üìç Executing action:', actionData);
        
        const context: VoiceActionContext = {
          userId: user?._id || '',
          userName: user?.name || '',
          userType: (user?.user_type as 'driver' | 'host') || 'driver',
          currentRoute: window.location.pathname + window.location.search,
          navigate,
        };

        const result = await executeAction(actionData.action, actionData.value, context);

        // Speak the action result or the Gemini response
        const spokenText = result.spoken || cleanResponse;
        if (spokenText) {
          addGenieMessage(spokenText);
        }

        // Navigate using React Router (sub-routes like /dashboard/trip-planner
        // are always different paths, so navigate() detects the change reliably)
        if (result.navigateTo) {
          setTimeout(() => {
            navigate(result.navigateTo!);
          }, 800);
        }
      } else if (cleanResponse) {
        // No action ‚Äî just a conversational response
        addGenieMessage(cleanResponse);
      }

      const lower = userInput.toLowerCase();
      const closeWords = ['bye', 'goodbye', 'good bye', 'close', 'exit', 'stop', 'shut up', 'go away', 'that\'s all', 'thanks bye', 'thank you bye', 'see you', 'later'];
      if (closeWords.some(w => lower.includes(w))) {
        setTimeout(() => {
          setIsOpen(false);
          setMessages([]);
          resetConversation();
          stopVAD();
        }, 4000);
      }

    } catch (error) {
      console.error('Error processing command:', error);
      addGenieMessage("I'm having trouble understanding. Could you try again?");
    }
  }, [user, navigate, addGenieMessage]);

  // Initialize Speech Recognition (supports Chrome, Edge, Safari, etc.)
  useEffect(() => {
    const SpeechRecognitionAPI = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    if (!SpeechRecognitionAPI) {
      console.warn('Speech recognition not supported in this browser');
      setMicSupported(false);
      return;
    }

    recognitionRef.current = new SpeechRecognitionAPI();
    recognitionRef.current.continuous = false;
    recognitionRef.current.interimResults = true;
    recognitionRef.current.lang = 'en-US';

    recognitionRef.current.onresult = (event: any) => {
      let interimTranscript = '';
      let finalTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      if (finalTranscript) {
        console.log('‚úÖ Final transcript:', finalTranscript);
        pendingTranscript.current = finalTranscript;
        setCurrentTranscript('');
      } else if (interimTranscript) {
        setCurrentTranscript(interimTranscript);
      }
    };

    recognitionRef.current.onerror = (event: any) => {
      console.log('Recognition error:', event.error);
      isRecognitionActive.current = false;
    };

    recognitionRef.current.onend = () => {
      console.log('Recognition ended');
      isRecognitionActive.current = false;
    };

    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (e) {
          // Already stopped
        }
      }
    };
  }, []);

  // Custom neural network wake word detection (like Alexa/Google)
  // Runs a TensorFlow.js model trained on YOUR voice, entirely in-browser
  useEffect(() => {
    let cleanedUp = false;

    const setup = async () => {
      const trained = await hasTrainedModel();
      if (!trained) {
        console.log('‚ÑπÔ∏è No wake word model trained yet. Use button or Ctrl+Shift+G.');
        return;
      }

      try {
        const success = await loadAndListen(() => {
          console.log('üåü Wake word detected! Opening Gennie...');
          if (!isOpenRef.current) {
            handleOpen();
          }
        });

        if (success && !cleanedUp) {
          setWakeWordReady(true);
          console.log('‚úÖ Custom wake word detection active - say "Hi Gennie"');
        }
      } catch (error) {
        console.warn('Wake word setup failed:', error);
      }
    };

    setup();

    return () => {
      cleanedUp = true;
      cleanupWakeWord();
      setWakeWordReady(false);
    };
  }, []);

  // Pause/resume wake word when Gennie opens/closes
  useEffect(() => {
    if (!wakeWordReady) return;
    if (isOpen) {
      pauseListening();
    } else {
      resumeListening();
    }
  }, [isOpen, wakeWordReady]);

  // Keyboard shortcut: Ctrl+Shift+G to toggle Gennie
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.ctrlKey && e.shiftKey && e.key === 'G') {
        e.preventDefault();
        if (isOpen) {
          handleClose();
        } else {
          handleOpen();
        }
      }
    };
    window.addEventListener('keydown', handleKeyDown);
    return () => window.removeEventListener('keydown', handleKeyDown);
  }, [isOpen]);

  // Initialize VAD when window opens
  useEffect(() => {
    if (!isOpen) return;

    console.log('üéØ Initializing VAD...');

    initVAD({
      onSpeechStart: () => {
        console.log('üë§ User started speaking');
        setIsUserSpeaking(true);
        
        // Start speech recognition when user starts speaking
        if (!isRecognitionActive.current && !isSpeaking && recognitionRef.current) {
          try {
            recognitionRef.current.start();
            isRecognitionActive.current = true;
            console.log('üé§ Started recognition');
          } catch (e) {
            console.log('Recognition already running');
          }
        }
      },
      
      onSpeechEnd: () => {
        console.log('üë§ User stopped speaking');
        setIsUserSpeaking(false);
        setCurrentTranscript('');
        
        // Stop recognition and process what was said
        if (isRecognitionActive.current && recognitionRef.current) {
          try {
            recognitionRef.current.stop();
          } catch (e) {
            // Already stopped
          }
        }

        // Process the transcript after a short delay
        setTimeout(async () => {
          if (pendingTranscript.current.trim() && !isSpeaking) {
            const text = pendingTranscript.current.trim();
            pendingTranscript.current = '';
            
            addUserMessage(text);
            setIsProcessing(true);
            await processCommand(text);
            setIsProcessing(false);
          }
        }, 500);
      },
      
      onVolumeChange: (volume) => {
        setAudioLevel(volume);
      }
    }).catch(error => {
      console.warn('VAD not available (mic denied or unsupported):', error);
      setVadSupported(false);
    });

    return () => {
      stopVAD();
    };
  }, [isOpen, isSpeaking, processCommand]);

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const handleClose = () => {
    setIsOpen(false);
    setMessages([]);
    setTextInput('');
    resetConversation();
    stopVAD();
  };

  const handleOpen = () => {
    setIsOpen(true);
    addGenieMessage("Hi! I'm Gennie, your ChargeNet assistant. How can I help you?");
  };

  // Manual tap-to-speak for mobile / when VAD doesn't work
  const handleManualMic = () => {
    if (!recognitionRef.current || isProcessing || isSpeaking) return;
    if (isRecognitionActive.current) {
      // Already listening ‚Äî stop it
      try { recognitionRef.current.stop(); } catch {}
      return;
    }
    try {
      recognitionRef.current.start();
      isRecognitionActive.current = true;
      setIsUserSpeaking(true);

      // Auto-stop after recognition ends and process
      const origOnEnd = recognitionRef.current.onend;
      recognitionRef.current.onend = () => {
        isRecognitionActive.current = false;
        setIsUserSpeaking(false);
        setCurrentTranscript('');

        setTimeout(async () => {
          if (pendingTranscript.current.trim() && !isSpeaking) {
            const text = pendingTranscript.current.trim();
            pendingTranscript.current = '';
            addUserMessage(text);
            setIsProcessing(true);
            await processCommand(text);
            setIsProcessing(false);
          }
        }, 300);
        // Restore original
        if (recognitionRef.current) recognitionRef.current.onend = origOnEnd;
      };
    } catch {
      console.log('Could not start recognition');
    }
  };

  // Text input submit
  const handleTextSubmit = async (e?: React.FormEvent) => {
    e?.preventDefault();
    const text = textInput.trim();
    if (!text || isProcessing) return;
    setTextInput('');
    addUserMessage(text);
    setIsProcessing(true);
    await processCommand(text);
    setIsProcessing(false);
  };

  return (
    <>
      {/* Wake Word Training Modal */}
      {showWakeWordSetup && (
        <WakeWordSetup
          onComplete={async () => {
            setShowWakeWordSetup(false);
            // Load and start listening with the freshly trained model
            const success = await loadAndListen(() => {
              if (!isOpenRef.current) handleOpen();
            });
            if (success) setWakeWordReady(true);
          }}
          onSkip={() => setShowWakeWordSetup(false)}
        />
      )}

      {/* Floating Button */}
      {!isOpen && (
        <div className="fixed bottom-6 right-6 z-50 flex flex-col items-center gap-2">
          {/* Train wake word button (small) */}
          <button
            onClick={() => setShowWakeWordSetup(true)}
            className="bg-white text-purple-600 p-2 rounded-full shadow-md hover:shadow-lg transition-all duration-300 border border-purple-200"
            title={wakeWordReady ? "Retrain wake word" : "Train wake word"}
          >
            <Settings className="h-4 w-4" />
          </button>
          <button
            onClick={handleOpen}
            className="bg-gradient-to-r from-purple-600 to-pink-600 text-white p-4 rounded-full shadow-lg hover:shadow-xl transition-all duration-300"
            title="Open Gennie (Ctrl+Shift+G)"
          >
            <Sparkles className="h-6 w-6" />
          </button>
          <p className="text-xs text-gray-600 text-center">
            {wakeWordReady ? 'üé§ Say "Hi Gennie"' : 'Ctrl+Shift+G'}
          </p>
        </div>
      )}

      {/* Chat Window */}
      {isOpen && (
        <div className="fixed bottom-6 right-6 w-96 h-[600px] bg-white rounded-2xl shadow-2xl flex flex-col z-50 border border-gray-200">
          {/* Header */}
          <div className="bg-gradient-to-r from-purple-600 to-pink-600 text-white p-4 rounded-t-2xl flex items-center justify-between">
            <div className="flex items-center gap-3">
              <div className="relative">
                {isSpeaking ? (
                  <Volume2 className="h-6 w-6 animate-pulse" />
                ) : isUserSpeaking ? (
                  <Mic className="h-6 w-6 animate-pulse" />
                ) : (
                  <Sparkles className="h-6 w-6" />
                )}
              </div>
              <div>
                <h3 className="font-semibold">Gennie</h3>
                <p className="text-xs opacity-90">
                  {isSpeaking ? 'üîä Speaking...' : isUserSpeaking ? 'üé§ Listening...' : '‚ú® Ready'}
                </p>
              </div>
            </div>
            <div className="flex items-center gap-1">
              <button
                onClick={async () => {
                  await resetWakeWord();
                  setWakeWordReady(false);
                  setShowWakeWordSetup(true);
                }}
                className="hover:bg-white/20 p-1 rounded-lg transition-colors"
                title="Retrain wake word"
              >
                <Settings className="h-4 w-4" />
              </button>
              <button
                onClick={handleClose}
                className="hover:bg-white/20 p-1 rounded-lg transition-colors"
              >
                <X className="h-5 w-5" />
              </button>
            </div>
          </div>

          {/* Messages */}
          <div className="flex-1 overflow-y-auto p-4 space-y-4 bg-gray-50">
            {messages.map((message) => (
              <div
                key={message.id}
                className={`flex ${message.sender === 'user' ? 'justify-end' : 'justify-start'}`}
              >
                <div
                  className={`max-w-[80%] p-3 rounded-2xl ${
                    message.sender === 'user'
                      ? 'bg-gradient-to-r from-purple-600 to-pink-600 text-white'
                      : 'bg-white text-gray-800 shadow-md border border-gray-200'
                  }`}
                >
                  <p className="text-sm whitespace-pre-line">{message.text}</p>
                  <p className={`text-xs mt-1 ${message.sender === 'user' ? 'text-purple-100' : 'text-gray-400'}`}>
                    {message.timestamp.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}
                  </p>
                </div>
              </div>
            ))}
            
            {/* Show current transcript while user is speaking */}
            {currentTranscript && (
              <div className="flex justify-end">
                <div className="max-w-[80%] p-3 rounded-2xl bg-purple-100 text-purple-800 border-2 border-purple-300">
                  <p className="text-sm italic">{currentTranscript}</p>
                </div>
              </div>
            )}
            
            {isProcessing && (
              <div className="flex justify-start">
                <div className="bg-white p-3 rounded-2xl shadow-md border border-gray-200">
                  <div className="flex gap-2">
                    <div className="w-2 h-2 bg-purple-600 rounded-full animate-bounce"></div>
                    <div className="w-2 h-2 bg-purple-600 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }}></div>
                    <div className="w-2 h-2 bg-purple-600 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }}></div>
                  </div>
                </div>
              </div>
            )}
            <div ref={messagesEndRef} />
          </div>

          {/* Input Area ‚Äî voice + text */}
          <div className="p-3 border-t border-gray-200 bg-white rounded-b-2xl">
            {/* Audio Level Visualizer (only when speaking) */}
            {(isUserSpeaking || isSpeaking) && (
              <div className="flex items-center justify-center gap-1 h-8 mb-2">
                {[...Array(5)].map((_, i) => {
                  const height = isUserSpeaking 
                    ? Math.min(32, (audioLevel / 100) * 32 * (1 - Math.abs(i - 2) * 0.2))
                    : 6;
                  return (
                    <div
                      key={i}
                      className="w-1.5 bg-purple-600 rounded-full transition-all duration-100"
                      style={{ height: `${height}px` }}
                    />
                  );
                })}
              </div>
            )}

            {/* Status line */}
            <p className="text-xs text-gray-400 text-center mb-2">
              {isSpeaking ? 'üîä Speaking...' : isUserSpeaking ? 'üé§ Listening...' : isProcessing ? 'ü§î Thinking...' : vadSupported ? 'üëÇ Speak or type below' : '‚å®Ô∏è Type your message'}
            </p>

            {/* Text input + mic + send */}
            <form onSubmit={handleTextSubmit} className="flex items-center gap-2">
              <input
                type="text"
                value={textInput}
                onChange={e => setTextInput(e.target.value)}
                placeholder="Type a message..."
                className="flex-1 px-3 py-2 text-sm border border-gray-300 rounded-full focus:outline-none focus:ring-2 focus:ring-purple-500 focus:border-transparent"
                disabled={isProcessing || isSpeaking}
              />
              {micSupported && (
                <button
                  type="button"
                  onClick={handleManualMic}
                  disabled={isProcessing || isSpeaking}
                  className={`p-2 rounded-full transition-all ${
                    isUserSpeaking 
                      ? 'bg-red-500 text-white animate-pulse' 
                      : 'bg-purple-100 text-purple-600 hover:bg-purple-200'
                  } disabled:opacity-50`}
                  title="Tap to speak"
                >
                  <Mic className="h-4 w-4" />
                </button>
              )}
              <button
                type="submit"
                disabled={!textInput.trim() || isProcessing || isSpeaking}
                className="p-2 bg-gradient-to-r from-purple-600 to-pink-600 text-white rounded-full disabled:opacity-50 hover:shadow-md transition-all"
                title="Send"
              >
                <Send className="h-4 w-4" />
              </button>
            </form>
          </div>
        </div>
      )}
    </>
  );
}
